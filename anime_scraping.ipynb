{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_review(review_url):\n",
    "    \"\"\"Fetch the full review from the given review URL.\"\"\"\n",
    "    review_response = requests.get(review_url)\n",
    "    review_soup = BeautifulSoup(review_response.content, 'html.parser')\n",
    "    full_review_element = review_soup.find(class_='p-mark__review')\n",
    "    return full_review_element.text.strip() if full_review_element else None\n",
    "\n",
    "def scrape_reviews(page_url):\n",
    "    \"\"\"Scrape reviews from the given page URL.\"\"\"\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with class 'p-mark'\n",
    "    p_mark_elements = soup.find_all(class_='p-mark')\n",
    "    \n",
    "    # Initialize lists to store scores and reviews\n",
    "    scores = []\n",
    "    reviews = []\n",
    "    \n",
    "    # Loop through each 'p-mark' element\n",
    "    for p_mark in p_mark_elements:\n",
    "        # Find the score within the 'p-mark' element\n",
    "        score_element = p_mark.find(class_='c-rating__score')\n",
    "        if score_element:\n",
    "            score_text = score_element.text.strip()\n",
    "            if score_text != '-':\n",
    "                try:\n",
    "                    scores.append(float(score_text))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Find the review within the 'p-mark' element\n",
    "        review_element = p_mark.find(class_='p-mark__review')\n",
    "        if review_element:\n",
    "            # Check if there is a \"続きを読む\" link\n",
    "            read_more_link = review_element.find('a')\n",
    "            if read_more_link and 'href' in read_more_link.attrs:\n",
    "                # Get the full review from the link\n",
    "                full_review_url = \"https://filmarks.com\" + read_more_link['href']\n",
    "                full_review = get_full_review(full_review_url)\n",
    "                reviews.append(full_review)\n",
    "            else:\n",
    "                reviews.append(review_element.text.strip())\n",
    "        else:\n",
    "            reviews.append(None)\n",
    "    \n",
    "    # Create a DataFrame from the lists of scores and reviews\n",
    "    df = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'review': reviews\n",
    "    })\n",
    "    \n",
    "    return df, soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL of the Filmarks anime page\n",
    "base_url = \"https://filmarks.com/animes/4206/5682\"\n",
    "\n",
    "# Initialize a list to store all DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Start with the first page\n",
    "page_url = base_url\n",
    "while page_url:\n",
    "    df, soup = scrape_reviews(page_url)\n",
    "    all_dfs.append(df)\n",
    "    \n",
    "    # Find the next page link\n",
    "    next_page_element = soup.find('a', class_='c2-pagination__next')\n",
    "    if next_page_element and 'href' in next_page_element.attrs:\n",
    "        next_page_url = next_page_element['href']\n",
    "        page_url = \"https://filmarks.com\" + next_page_url\n",
    "    else:\n",
    "        page_url = None\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>はじめはギャグテイストのアニメだと思いきや話数が進んでいくと、一人一人の過去の掘り下げが面白...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.4</td>\n",
       "      <td>宮野真守氏の演技の振り幅を堪能するアニメだった笑 スポ根アニメ好きとしては押さえておこうと見...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.3</td>\n",
       "      <td>漫画通り！まもちゃんさすがです。ライラックもアニメと合ってて良かったな</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.8</td>\n",
       "      <td>胸熱灼熱面白かったー！憧れの背中を押すという言葉通り、清峰・要バッテリーの前に一度夢を諦めた...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.1</td>\n",
       "      <td>1期完走！野球大好きなので見始めたけど、思ったよりギャグ要素強くてめっちゃ笑ってた＾＾おバカ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>4.5</td>\n",
       "      <td>ジャンプラで一番好きな作品アニメも最高この作品は、シリアスとコメディの温度差で風邪引いてしま...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>3.0</td>\n",
       "      <td>絶対ノートまで辿りつかないのか〜…２期はいつからですか😂ＯＰすごい良かったイントロのリフが印...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>4.5</td>\n",
       "      <td>今シーズンのエース級作品⚾️🔥エンディング曲のマカえんもよかった</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>4.0</td>\n",
       "      <td>声優もイメージ相違なくて最後まで見るのが楽しみ。オープニング爽やかでとっても良い</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>3.7</td>\n",
       "      <td>期待度5(キャラデザは独特だがクオリティは高い。どこか古くささを感じる青春ストーリーに期待)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     score                                             review\n",
       "0      4.5  はじめはギャグテイストのアニメだと思いきや話数が進んでいくと、一人一人の過去の掘り下げが面白...\n",
       "1      3.4  宮野真守氏の演技の振り幅を堪能するアニメだった笑 スポ根アニメ好きとしては押さえておこうと見...\n",
       "2      4.3                漫画通り！まもちゃんさすがです。ライラックもアニメと合ってて良かったな\n",
       "3      4.8  胸熱灼熱面白かったー！憧れの背中を押すという言葉通り、清峰・要バッテリーの前に一度夢を諦めた...\n",
       "4      4.1  1期完走！野球大好きなので見始めたけど、思ったよりギャグ要素強くてめっちゃ笑ってた＾＾おバカ...\n",
       "..     ...                                                ...\n",
       "513    4.5  ジャンプラで一番好きな作品アニメも最高この作品は、シリアスとコメディの温度差で風邪引いてしま...\n",
       "514    3.0  絶対ノートまで辿りつかないのか〜…２期はいつからですか😂ＯＰすごい良かったイントロのリフが印...\n",
       "515    4.5                   今シーズンのエース級作品⚾️🔥エンディング曲のマカえんもよかった\n",
       "516    4.0           声優もイメージ相違なくて最後まで見るのが楽しみ。オープニング爽やかでとっても良い\n",
       "517    3.7  期待度5(キャラデザは独特だがクオリティは高い。どこか古くささを感じる青春ストーリーに期待)...\n",
       "\n",
       "[518 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from janome.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 日本語フォントを表示するための設定値\n",
    "fpath = '/System/Library/Fonts/ヒラギノ丸ゴ ProN W4.ttc'\n",
    "\n",
    "def get_full_review(review_url):\n",
    "    \"\"\"Fetch the full review from the given review URL.\"\"\"\n",
    "    review_response = requests.get(review_url)\n",
    "    review_soup = BeautifulSoup(review_response.content, 'html.parser')\n",
    "    full_review_element = review_soup.find(class_='p-mark__review')\n",
    "    return full_review_element.text.strip() if full_review_element else None\n",
    "\n",
    "def scrape_reviews(page_url):\n",
    "    \"\"\"Scrape reviews from the given page URL.\"\"\"\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with class 'p-mark'\n",
    "    p_mark_elements = soup.find_all(class_='p-mark')\n",
    "    \n",
    "    # Initialize lists to store scores and reviews\n",
    "    scores = []\n",
    "    reviews = []\n",
    "    \n",
    "    # Loop through each 'p-mark' element\n",
    "    for p_mark in p_mark_elements:\n",
    "        # Find the score within the 'p-mark' element\n",
    "        score_element = p_mark.find(class_='c-rating__score')\n",
    "        if score_element:\n",
    "            score_text = score_element.text.strip()\n",
    "            if score_text != '-':\n",
    "                try:\n",
    "                    score = float(score_text)\n",
    "                    scores.append(score)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Find the review within the 'p-mark' element\n",
    "        review_element = p_mark.find(class_='p-mark__review')\n",
    "        if review_element:\n",
    "            # Check if there is a \"続きを読む\" link\n",
    "            read_more_link = review_element.find('a')\n",
    "            if read_more_link and 'href' in read_more_link.attrs:\n",
    "                # Get the full review from the link\n",
    "                full_review_url = \"https://filmarks.com\" + read_more_link['href']\n",
    "                full_review = get_full_review(full_review_url)\n",
    "                reviews.append(full_review)\n",
    "            else:\n",
    "                reviews.append(review_element.text.strip())\n",
    "        else:\n",
    "            reviews.append(None)\n",
    "    \n",
    "    # Create a DataFrame from the lists of scores and reviews\n",
    "    df = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'review': reviews\n",
    "    })\n",
    "    \n",
    "    return df, soup\n",
    "\n",
    "def scrape_all_reviews(base_url):\n",
    "    # Initialize a list to store all DataFrames\n",
    "    all_dfs = []\n",
    "\n",
    "    # Start with the first page\n",
    "    page_url = base_url\n",
    "    while page_url:\n",
    "        df, soup = scrape_reviews(page_url)\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "        # Find the next page link\n",
    "        next_page_element = soup.find('a', class_='c2-pagination__next')\n",
    "        if next_page_element and 'href' in next_page_element.attrs:\n",
    "            next_page_url = next_page_element['href']\n",
    "            page_url = \"https://filmarks.com\" + next_page_url\n",
    "        else:\n",
    "            page_url = None\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# Input\n",
    "base_url = st.text_input('ベースとなるURLを入力してください', placeholder='（例）https://filmarks.com/animes/4206/5682')\n",
    "if 'data' not in st.session_state:\n",
    "    st.session_state.data = None\n",
    "\n",
    "if st.button('スクレイピングを実行'):\n",
    "    with st.spinner('スクレイピング中...'):\n",
    "        df = scrape_all_reviews(base_url)\n",
    "        st.session_state.data = df\n",
    "        st.success('スクレイピング完了')\n",
    "\n",
    "if st.session_state.data is not None:\n",
    "    df = st.session_state.data\n",
    "    \n",
    "    score_range = st.selectbox('スコア範囲を選択', \n",
    "                               ['0-1', '1-2', '2-3', '3-4', '4-5'])\n",
    "\n",
    "    # 品詞を選択（複数選択）\n",
    "    word_class = st.multiselect('品詞を選択',['名詞','形容詞','動詞','副詞'])\n",
    "\n",
    "    # 指定した単語をストップワードとして除外\n",
    "    stop_text = st.text_input(\"カンマ区切りでストップワードを設定\")\n",
    "    stop_list = [x.strip() for x in stop_text.split(\",\")]\n",
    "\n",
    "    # Process\n",
    "    if st.button('ワードクラウドを作成'):\n",
    "        # スコア範囲でフィルタリング\n",
    "        score_min, score_max = map(float, score_range.split('-'))\n",
    "        filtered_df = df[(df['score'] >= score_min) & (df['score'] < score_max)]\n",
    "        \n",
    "        # Noneを空文字列に置き換え\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df.loc[:, 'review'] = filtered_df['review'].apply(lambda x: x if x is not None else '')\n",
    "        \n",
    "        # レビューのテキストを連結\n",
    "        input_text = ' '.join(filtered_df['review'])\n",
    "        \n",
    "        \n",
    "        word_list = [] #分割後の形態素を一次格納する空のリストを用意\n",
    "        \n",
    "        for token in Tokenizer().tokenize(input_text):\n",
    "            split_token = token.part_of_speech.split(',')\n",
    "            if split_token[0] in word_class:\n",
    "                word_list.append(token.base_form)\n",
    "    \n",
    "\n",
    "        if word_list:\n",
    "            # 単語の頻度集計\n",
    "            df_freq = pd.DataFrame(word_list, columns=['単語'])\n",
    "            df_freq['回数'] = 1  \n",
    "            df_freq_sorted = df_freq.groupby('単語').sum().reset_index().sort_values('回数', ascending=False)\n",
    "\n",
    "            # インデックスをリセット\n",
    "            df_freq_sorted.reset_index(inplace=True, drop=True)\n",
    "            # 単語の先頭にインデックス番号を挿入\n",
    "            df_freq_sorted['単語（ソート用）'] = df_freq_sorted.index.astype(str).str.zfill(2) + '_' + df_freq_sorted['単語']\n",
    "            \n",
    "            ### word cloud作成 ###\n",
    "\n",
    "            # word cloud側の仕様で、単語リストの要素を空白区切りに連結する\n",
    "            word_space = ' '.join(map(str, word_list)) #数字が入っていた場合の対策として、strに変換してから連結\n",
    "\n",
    "            # word cloudの設定(フォントの設定)\n",
    "            wc = WordCloud(background_color='white', colormap='summer', font_path=fpath, width=800, height=400, stopwords=stop_list)\n",
    "                                                              # オプション stopwords=[単語リスト]で除外対象単語（ストップワード）を設定可能\n",
    "                                                              # 設定例：stopwords=['テレビ', '商品']\n",
    "            wc.generate(word_space)\n",
    "            \n",
    "            ##出力画像の大きさの指定\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            ## 目盛り削除など見た目の修正\n",
    "            plt.tick_params(labelbottom=False,\n",
    "                            labelleft=False,\n",
    "                            labelright=False,\n",
    "                            labeltop=False,\n",
    "                            length=0)\n",
    "\n",
    "            st.image(wc.to_array())\n",
    "        else:\n",
    "            st.write('指定された品詞に一致する単語がありませんでした。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.96.125.141:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "2024-07-19 12:32:19.348 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuzawakenta/Library/Python/3.10/lib/python/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 584, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/yuzawakenta/vscode/task/classes/情報処理B/class_code/app.py\", line 182, in <module>\n",
      "    word_counts = {word: word_list.count(word) for word in target_words}\n",
      "  File \"/Users/yuzawakenta/vscode/task/classes/情報処理B/class_code/app.py\", line 182, in <dictcomp>\n",
      "    word_counts = {word: word_list.count(word) for word in target_words}\n",
      "NameError: name 'word_list' is not defined\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!echo | streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from janome.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 日本語フォントを表示するための設定値\n",
    "fpath = '/System/Library/Fonts/ヒラギノ丸ゴ ProN W4.ttc'\n",
    "\n",
    "def get_full_review(review_url):\n",
    "    \"\"\"Fetch the full review from the given review URL.\"\"\"\n",
    "    review_response = requests.get(review_url)\n",
    "    review_soup = BeautifulSoup(review_response.content, 'html.parser')\n",
    "    full_review_element = review_soup.find(class_='p-mark__review')\n",
    "    return full_review_element.text.strip() if full_review_element else None\n",
    "\n",
    "def scrape_reviews(page_url):\n",
    "    \"\"\"Scrape reviews from the given page URL.\"\"\"\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with class 'p-mark'\n",
    "    p_mark_elements = soup.find_all(class_='p-mark')\n",
    "    \n",
    "    # Initialize lists to store scores and reviews\n",
    "    scores = []\n",
    "    reviews = []\n",
    "    \n",
    "    # Loop through each 'p-mark' element\n",
    "    for p_mark in p_mark_elements:\n",
    "        # Find the score within the 'p-mark' element\n",
    "        score_element = p_mark.find(class_='c-rating__score')\n",
    "        if score_element:\n",
    "            score_text = score_element.text.strip()\n",
    "            if score_text != '-':\n",
    "                try:\n",
    "                    score = float(score_text)\n",
    "                    scores.append(score)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Find the review within the 'p-mark' element\n",
    "        review_element = p_mark.find(class_='p-mark__review')\n",
    "        if review_element:\n",
    "            # Check if there is a \"続きを読む\" link\n",
    "            read_more_link = review_element.find('a')\n",
    "            if read_more_link and 'href' in read_more_link.attrs:\n",
    "                # Get the full review from the link\n",
    "                full_review_url = \"https://filmarks.com\" + read_more_link['href']\n",
    "                full_review = get_full_review(full_review_url)\n",
    "                reviews.append(full_review)\n",
    "            else:\n",
    "                reviews.append(review_element.text.strip())\n",
    "        else:\n",
    "            reviews.append(None)\n",
    "    \n",
    "    # Create a DataFrame from the lists of scores and reviews\n",
    "    df = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'review': reviews\n",
    "    })\n",
    "    \n",
    "    return df, soup\n",
    "\n",
    "def scrape_all_reviews(base_url):\n",
    "    # Initialize a list to store all DataFrames\n",
    "    all_dfs = []\n",
    "\n",
    "    # Start with the first page\n",
    "    page_url = base_url\n",
    "    while page_url:\n",
    "        df, soup = scrape_reviews(page_url)\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "        # Find the next page link\n",
    "        next_page_element = soup.find('a', class_='c2-pagination__next')\n",
    "        if next_page_element and 'href' in next_page_element.attrs:\n",
    "            next_page_url = next_page_element['href']\n",
    "            page_url = \"https://filmarks.com\" + next_page_url\n",
    "        else:\n",
    "            page_url = None\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# Input\n",
    "base_url = st.text_input('ベースとなるURLを入力してください', placeholder='（例）https://filmarks.com/animes/4206/5682')\n",
    "if 'data' not in st.session_state:\n",
    "    st.session_state.data = None\n",
    "\n",
    "if st.button('スクレイピングを実行'):\n",
    "    with st.spinner('スクレイピング中...'):\n",
    "        df = scrape_all_reviews(base_url)\n",
    "        st.session_state.data = df\n",
    "        st.success('スクレイピング完了')\n",
    "\n",
    "if st.session_state.data is not None:\n",
    "    df = st.session_state.data\n",
    "    \n",
    "    score_range = st.selectbox('スコア範囲を選択', \n",
    "                               ['0-1', '1-2', '2-3', '3-4', '4-5'])\n",
    "\n",
    "    # 品詞を選択（複数選択）\n",
    "    word_class = st.multiselect('品詞を選択',['名詞','形容詞','動詞','副詞'])\n",
    "\n",
    "    # 指定した単語をストップワードとして除外\n",
    "    stop_text = st.text_input(\"カンマ区切りでストップワードを設定\")\n",
    "    stop_list = [x.strip() for x in stop_text.split(\",\")]\n",
    "\n",
    "    # 単語群をカンマ区切りで入力\n",
    "    target_words_text = st.text_input(\"カンマ区切りでターゲットとなる単語群を入力\")\n",
    "    target_words = [x.strip() for x in target_words_text.split(\",\")]\n",
    "\n",
    "    if st.button('データ処理'):\n",
    "        # スコア範囲でフィルタリング\n",
    "        score_min, score_max = map(float, score_range.split('-'))\n",
    "        filtered_df = df[(df['score'] >= score_min) & (df['score'] < score_max)]\n",
    "        \n",
    "        # Noneを空文字列に置き換え\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df.loc[:, 'review'] = filtered_df['review'].apply(lambda x: x if x is not None else '')\n",
    "        \n",
    "        # レビューのテキストを連結\n",
    "        input_text = ' '.join(filtered_df['review'])\n",
    "\n",
    "        \n",
    "        word_list = [] #分割後の形態素を一次格納する空のリストを用意\n",
    "        \n",
    "        for token in Tokenizer().tokenize(input_text):\n",
    "            split_token = token.part_of_speech.split(',')\n",
    "            if split_token[0] in word_class:\n",
    "                word_list.append(token.base_form)\n",
    "        \n",
    "\n",
    "        if target_words:\n",
    "            # 各ターゲット単語の出現回数をカウント\n",
    "            word_counts = {word: word_list.count(word) for word in target_words}\n",
    "\n",
    "            # データフレームに変換\n",
    "            df_counts = pd.DataFrame(list(word_counts.items()), columns=['単語', '回数'])\n",
    "\n",
    "            # 横棒グラフを作成\n",
    "            st.bar_chart(df_counts.set_index('単語'))\n",
    "\n",
    "        if word_list:\n",
    "            # 単語の頻度集計\n",
    "            df_freq = pd.DataFrame(word_list, columns=['単語'])\n",
    "            df_freq['回数'] = 1  \n",
    "            df_freq_sorted = df_freq.groupby('単語').sum().reset_index().sort_values('回数', ascending=False)\n",
    "\n",
    "            # インデックスをリセット\n",
    "            df_freq_sorted.reset_index(inplace=True, drop=True)\n",
    "            # 単語の先頭にインデックス番号を挿入\n",
    "            df_freq_sorted['単語（ソート用）'] = df_freq_sorted.index.astype(str).str.zfill(2) + '_' + df_freq_sorted['単語']\n",
    "            \n",
    "            ### word cloud作成 ###\n",
    "\n",
    "            # word cloud側の仕様で、単語リストの要素を空白区切りに連結する\n",
    "            word_space = ' '.join(map(str, word_list)) #数字が入っていた場合の対策として、strに変換してから連結\n",
    "\n",
    "            # word cloudの設定(フォントの設定)\n",
    "            wc = WordCloud(background_color='white', colormap='summer', font_path=fpath, width=800, height=400, stopwords=stop_list)\n",
    "                                                              # オプション stopwords=[単語リスト]で除外対象単語（ストップワード）を設定可能\n",
    "                                                              # 設定例：stopwords=['テレビ', '商品']\n",
    "            wc.generate(word_space)\n",
    "            \n",
    "            ##出力画像の大きさの指定\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            ## 目盛り削除など見た目の修正\n",
    "            plt.tick_params(labelbottom=False,\n",
    "                            labelleft=False,\n",
    "                            labelright=False,\n",
    "                            labeltop=False,\n",
    "                            length=0)\n",
    "\n",
    "            st.image(wc.to_array())\n",
    "        else:\n",
    "            st.write('指定された品詞に一致する単語がありませんでした。')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.96.125.141:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!echo | streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
