{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_review(review_url):\n",
    "    \"\"\"Fetch the full review from the given review URL.\"\"\"\n",
    "    review_response = requests.get(review_url)\n",
    "    review_soup = BeautifulSoup(review_response.content, 'html.parser')\n",
    "    full_review_element = review_soup.find(class_='p-mark__review')\n",
    "    return full_review_element.text.strip() if full_review_element else None\n",
    "\n",
    "def scrape_reviews(page_url):\n",
    "    \"\"\"Scrape reviews from the given page URL.\"\"\"\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with class 'p-mark'\n",
    "    p_mark_elements = soup.find_all(class_='p-mark')\n",
    "    \n",
    "    # Initialize lists to store scores and reviews\n",
    "    scores = []\n",
    "    reviews = []\n",
    "    \n",
    "    # Loop through each 'p-mark' element\n",
    "    for p_mark in p_mark_elements:\n",
    "        # Find the score within the 'p-mark' element\n",
    "        score_element = p_mark.find(class_='c-rating__score')\n",
    "        if score_element:\n",
    "            score_text = score_element.text.strip()\n",
    "            if score_text != '-':\n",
    "                try:\n",
    "                    scores.append(float(score_text))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        # Find the review within the 'p-mark' element\n",
    "        review_element = p_mark.find(class_='p-mark__review')\n",
    "        if review_element:\n",
    "            # Check if there is a \"ç¶šãã‚’èª­ã‚€\" link\n",
    "            read_more_link = review_element.find('a')\n",
    "            if read_more_link and 'href' in read_more_link.attrs:\n",
    "                # Get the full review from the link\n",
    "                full_review_url = \"https://filmarks.com\" + read_more_link['href']\n",
    "                full_review = get_full_review(full_review_url)\n",
    "                reviews.append(full_review)\n",
    "            else:\n",
    "                reviews.append(review_element.text.strip())\n",
    "        else:\n",
    "            reviews.append(None)\n",
    "    \n",
    "    # Create a DataFrame from the lists of scores and reviews\n",
    "    df = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'review': reviews\n",
    "    })\n",
    "    \n",
    "    return df, soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base URL of the Filmarks anime page\n",
    "base_url = \"https://filmarks.com/animes/4206/5682\"\n",
    "\n",
    "# Initialize a list to store all DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Start with the first page\n",
    "page_url = base_url\n",
    "while page_url:\n",
    "    df, soup = scrape_reviews(page_url)\n",
    "    all_dfs.append(df)\n",
    "    \n",
    "    # Find the next page link\n",
    "    next_page_element = soup.find('a', class_='c2-pagination__next')\n",
    "    if next_page_element and 'href' in next_page_element.attrs:\n",
    "        next_page_url = next_page_element['href']\n",
    "        page_url = \"https://filmarks.com\" + next_page_url\n",
    "    else:\n",
    "        page_url = None\n",
    "\n",
    "# Concatenate all DataFrames into one\n",
    "final_df = pd.concat(all_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.5</td>\n",
       "      <td>ã¯ã˜ã‚ã¯ã‚®ãƒ£ã‚°ãƒ†ã‚¤ã‚¹ãƒˆã®ã‚¢ãƒ‹ãƒ¡ã ã¨æ€ã„ãã‚„è©±æ•°ãŒé€²ã‚“ã§ã„ãã¨ã€ä¸€äººä¸€äººã®éå»ã®æ˜ã‚Šä¸‹ã’ãŒé¢ç™½...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.4</td>\n",
       "      <td>å®®é‡çœŸå®ˆæ°ã®æ¼”æŠ€ã®æŒ¯ã‚Šå¹…ã‚’å ªèƒ½ã™ã‚‹ã‚¢ãƒ‹ãƒ¡ã ã£ãŸç¬‘ ã‚¹ãƒæ ¹ã‚¢ãƒ‹ãƒ¡å¥½ãã¨ã—ã¦ã¯æŠ¼ã•ãˆã¦ãŠã“ã†ã¨è¦‹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.3</td>\n",
       "      <td>æ¼«ç”»é€šã‚Šï¼ã¾ã‚‚ã¡ã‚ƒã‚“ã•ã™ãŒã§ã™ã€‚ãƒ©ã‚¤ãƒ©ãƒƒã‚¯ã‚‚ã‚¢ãƒ‹ãƒ¡ã¨åˆã£ã¦ã¦è‰¯ã‹ã£ãŸãª</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.8</td>\n",
       "      <td>èƒ¸ç†±ç¼ç†±é¢ç™½ã‹ã£ãŸãƒ¼ï¼æ†§ã‚Œã®èƒŒä¸­ã‚’æŠ¼ã™ã¨ã„ã†è¨€è‘‰é€šã‚Šã€æ¸…å³°ãƒ»è¦ãƒãƒƒãƒ†ãƒªãƒ¼ã®å‰ã«ä¸€åº¦å¤¢ã‚’è«¦ã‚ãŸ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.1</td>\n",
       "      <td>1æœŸå®Œèµ°ï¼é‡çƒå¤§å¥½ããªã®ã§è¦‹å§‹ã‚ãŸã‘ã©ã€æ€ã£ãŸã‚ˆã‚Šã‚®ãƒ£ã‚°è¦ç´ å¼·ãã¦ã‚ã£ã¡ã‚ƒç¬‘ã£ã¦ãŸï¼¾ï¼¾ãŠãƒã‚«...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>4.5</td>\n",
       "      <td>ã‚¸ãƒ£ãƒ³ãƒ—ãƒ©ã§ä¸€ç•ªå¥½ããªä½œå“ã‚¢ãƒ‹ãƒ¡ã‚‚æœ€é«˜ã“ã®ä½œå“ã¯ã€ã‚·ãƒªã‚¢ã‚¹ã¨ã‚³ãƒ¡ãƒ‡ã‚£ã®æ¸©åº¦å·®ã§é¢¨é‚ªå¼•ã„ã¦ã—ã¾...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>3.0</td>\n",
       "      <td>çµ¶å¯¾ãƒãƒ¼ãƒˆã¾ã§è¾¿ã‚Šã¤ã‹ãªã„ã®ã‹ã€œâ€¦ï¼’æœŸã¯ã„ã¤ã‹ã‚‰ã§ã™ã‹ğŸ˜‚ï¼¯ï¼°ã™ã”ã„è‰¯ã‹ã£ãŸã‚¤ãƒ³ãƒˆãƒ­ã®ãƒªãƒ•ãŒå°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>4.5</td>\n",
       "      <td>ä»Šã‚·ãƒ¼ã‚ºãƒ³ã®ã‚¨ãƒ¼ã‚¹ç´šä½œå“âš¾ï¸ğŸ”¥ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ›²ã®ãƒã‚«ãˆã‚“ã‚‚ã‚ˆã‹ã£ãŸ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>4.0</td>\n",
       "      <td>å£°å„ªã‚‚ã‚¤ãƒ¡ãƒ¼ã‚¸ç›¸é•ãªãã¦æœ€å¾Œã¾ã§è¦‹ã‚‹ã®ãŒæ¥½ã—ã¿ã€‚ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°çˆ½ã‚„ã‹ã§ã¨ã£ã¦ã‚‚è‰¯ã„</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>3.7</td>\n",
       "      <td>æœŸå¾…åº¦5(ã‚­ãƒ£ãƒ©ãƒ‡ã‚¶ã¯ç‹¬ç‰¹ã ãŒã‚¯ã‚ªãƒªãƒ†ã‚£ã¯é«˜ã„ã€‚ã©ã“ã‹å¤ãã•ã•ã‚’æ„Ÿã˜ã‚‹é’æ˜¥ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã«æœŸå¾…)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>518 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     score                                             review\n",
       "0      4.5  ã¯ã˜ã‚ã¯ã‚®ãƒ£ã‚°ãƒ†ã‚¤ã‚¹ãƒˆã®ã‚¢ãƒ‹ãƒ¡ã ã¨æ€ã„ãã‚„è©±æ•°ãŒé€²ã‚“ã§ã„ãã¨ã€ä¸€äººä¸€äººã®éå»ã®æ˜ã‚Šä¸‹ã’ãŒé¢ç™½...\n",
       "1      3.4  å®®é‡çœŸå®ˆæ°ã®æ¼”æŠ€ã®æŒ¯ã‚Šå¹…ã‚’å ªèƒ½ã™ã‚‹ã‚¢ãƒ‹ãƒ¡ã ã£ãŸç¬‘ ã‚¹ãƒæ ¹ã‚¢ãƒ‹ãƒ¡å¥½ãã¨ã—ã¦ã¯æŠ¼ã•ãˆã¦ãŠã“ã†ã¨è¦‹...\n",
       "2      4.3                æ¼«ç”»é€šã‚Šï¼ã¾ã‚‚ã¡ã‚ƒã‚“ã•ã™ãŒã§ã™ã€‚ãƒ©ã‚¤ãƒ©ãƒƒã‚¯ã‚‚ã‚¢ãƒ‹ãƒ¡ã¨åˆã£ã¦ã¦è‰¯ã‹ã£ãŸãª\n",
       "3      4.8  èƒ¸ç†±ç¼ç†±é¢ç™½ã‹ã£ãŸãƒ¼ï¼æ†§ã‚Œã®èƒŒä¸­ã‚’æŠ¼ã™ã¨ã„ã†è¨€è‘‰é€šã‚Šã€æ¸…å³°ãƒ»è¦ãƒãƒƒãƒ†ãƒªãƒ¼ã®å‰ã«ä¸€åº¦å¤¢ã‚’è«¦ã‚ãŸ...\n",
       "4      4.1  1æœŸå®Œèµ°ï¼é‡çƒå¤§å¥½ããªã®ã§è¦‹å§‹ã‚ãŸã‘ã©ã€æ€ã£ãŸã‚ˆã‚Šã‚®ãƒ£ã‚°è¦ç´ å¼·ãã¦ã‚ã£ã¡ã‚ƒç¬‘ã£ã¦ãŸï¼¾ï¼¾ãŠãƒã‚«...\n",
       "..     ...                                                ...\n",
       "513    4.5  ã‚¸ãƒ£ãƒ³ãƒ—ãƒ©ã§ä¸€ç•ªå¥½ããªä½œå“ã‚¢ãƒ‹ãƒ¡ã‚‚æœ€é«˜ã“ã®ä½œå“ã¯ã€ã‚·ãƒªã‚¢ã‚¹ã¨ã‚³ãƒ¡ãƒ‡ã‚£ã®æ¸©åº¦å·®ã§é¢¨é‚ªå¼•ã„ã¦ã—ã¾...\n",
       "514    3.0  çµ¶å¯¾ãƒãƒ¼ãƒˆã¾ã§è¾¿ã‚Šã¤ã‹ãªã„ã®ã‹ã€œâ€¦ï¼’æœŸã¯ã„ã¤ã‹ã‚‰ã§ã™ã‹ğŸ˜‚ï¼¯ï¼°ã™ã”ã„è‰¯ã‹ã£ãŸã‚¤ãƒ³ãƒˆãƒ­ã®ãƒªãƒ•ãŒå°...\n",
       "515    4.5                   ä»Šã‚·ãƒ¼ã‚ºãƒ³ã®ã‚¨ãƒ¼ã‚¹ç´šä½œå“âš¾ï¸ğŸ”¥ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°æ›²ã®ãƒã‚«ãˆã‚“ã‚‚ã‚ˆã‹ã£ãŸ\n",
       "516    4.0           å£°å„ªã‚‚ã‚¤ãƒ¡ãƒ¼ã‚¸ç›¸é•ãªãã¦æœ€å¾Œã¾ã§è¦‹ã‚‹ã®ãŒæ¥½ã—ã¿ã€‚ã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°çˆ½ã‚„ã‹ã§ã¨ã£ã¦ã‚‚è‰¯ã„\n",
       "517    3.7  æœŸå¾…åº¦5(ã‚­ãƒ£ãƒ©ãƒ‡ã‚¶ã¯ç‹¬ç‰¹ã ãŒã‚¯ã‚ªãƒªãƒ†ã‚£ã¯é«˜ã„ã€‚ã©ã“ã‹å¤ãã•ã•ã‚’æ„Ÿã˜ã‚‹é’æ˜¥ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã«æœŸå¾…)...\n",
       "\n",
       "[518 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# å®Ÿè£…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from janome.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®è¨­å®šå€¤\n",
    "fpath = '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒä¸¸ã‚´ ProN W4.ttc'\n",
    "\n",
    "def get_full_review(review_url):\n",
    "    \"\"\"Fetch the full review from the given review URL.\"\"\"\n",
    "    review_response = requests.get(review_url)\n",
    "    review_soup = BeautifulSoup(review_response.content, 'html.parser')\n",
    "    full_review_element = review_soup.find(class_='p-mark__review')\n",
    "    return full_review_element.text.strip() if full_review_element else None\n",
    "\n",
    "def scrape_reviews(page_url):\n",
    "    \"\"\"Scrape reviews from the given page URL.\"\"\"\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with class 'p-mark'\n",
    "    p_mark_elements = soup.find_all(class_='p-mark')\n",
    "    \n",
    "    # Initialize lists to store scores and reviews\n",
    "    scores = []\n",
    "    reviews = []\n",
    "    \n",
    "    # Loop through each 'p-mark' element\n",
    "    for p_mark in p_mark_elements:\n",
    "        # Find the score within the 'p-mark' element\n",
    "        score_element = p_mark.find(class_='c-rating__score')\n",
    "        if score_element:\n",
    "            score_text = score_element.text.strip()\n",
    "            if score_text != '-':\n",
    "                try:\n",
    "                    score = float(score_text)\n",
    "                    scores.append(score)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Find the review within the 'p-mark' element\n",
    "        review_element = p_mark.find(class_='p-mark__review')\n",
    "        if review_element:\n",
    "            # Check if there is a \"ç¶šãã‚’èª­ã‚€\" link\n",
    "            read_more_link = review_element.find('a')\n",
    "            if read_more_link and 'href' in read_more_link.attrs:\n",
    "                # Get the full review from the link\n",
    "                full_review_url = \"https://filmarks.com\" + read_more_link['href']\n",
    "                full_review = get_full_review(full_review_url)\n",
    "                reviews.append(full_review)\n",
    "            else:\n",
    "                reviews.append(review_element.text.strip())\n",
    "        else:\n",
    "            reviews.append(None)\n",
    "    \n",
    "    # Create a DataFrame from the lists of scores and reviews\n",
    "    df = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'review': reviews\n",
    "    })\n",
    "    \n",
    "    return df, soup\n",
    "\n",
    "def scrape_all_reviews(base_url):\n",
    "    # Initialize a list to store all DataFrames\n",
    "    all_dfs = []\n",
    "\n",
    "    # Start with the first page\n",
    "    page_url = base_url\n",
    "    while page_url:\n",
    "        df, soup = scrape_reviews(page_url)\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "        # Find the next page link\n",
    "        next_page_element = soup.find('a', class_='c2-pagination__next')\n",
    "        if next_page_element and 'href' in next_page_element.attrs:\n",
    "            next_page_url = next_page_element['href']\n",
    "            page_url = \"https://filmarks.com\" + next_page_url\n",
    "        else:\n",
    "            page_url = None\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# Input\n",
    "base_url = st.text_input('ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', placeholder='ï¼ˆä¾‹ï¼‰https://filmarks.com/animes/4206/5682')\n",
    "if 'data' not in st.session_state:\n",
    "    st.session_state.data = None\n",
    "\n",
    "if st.button('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ'):\n",
    "    with st.spinner('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...'):\n",
    "        df = scrape_all_reviews(base_url)\n",
    "        st.session_state.data = df\n",
    "        st.success('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†')\n",
    "\n",
    "if st.session_state.data is not None:\n",
    "    df = st.session_state.data\n",
    "    \n",
    "    score_range = st.selectbox('ã‚¹ã‚³ã‚¢ç¯„å›²ã‚’é¸æŠ', \n",
    "                               ['0-1', '1-2', '2-3', '3-4', '4-5'])\n",
    "\n",
    "    # å“è©ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠï¼‰\n",
    "    word_class = st.multiselect('å“è©ã‚’é¸æŠ',['åè©','å½¢å®¹è©','å‹•è©','å‰¯è©'])\n",
    "\n",
    "    # æŒ‡å®šã—ãŸå˜èªã‚’ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦é™¤å¤–\n",
    "    stop_text = st.text_input(\"ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®š\")\n",
    "    stop_list = [x.strip() for x in stop_text.split(\",\")]\n",
    "\n",
    "    # Process\n",
    "    if st.button('ãƒ¯ãƒ¼ãƒ‰ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ä½œæˆ'):\n",
    "        # ã‚¹ã‚³ã‚¢ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        score_min, score_max = map(float, score_range.split('-'))\n",
    "        filtered_df = df[(df['score'] >= score_min) & (df['score'] < score_max)]\n",
    "        \n",
    "        # Noneã‚’ç©ºæ–‡å­—åˆ—ã«ç½®ãæ›ãˆ\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df.loc[:, 'review'] = filtered_df['review'].apply(lambda x: x if x is not None else '')\n",
    "        \n",
    "        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é€£çµ\n",
    "        input_text = ' '.join(filtered_df['review'])\n",
    "        \n",
    "        \n",
    "        word_list = [] #åˆ†å‰²å¾Œã®å½¢æ…‹ç´ ã‚’ä¸€æ¬¡æ ¼ç´ã™ã‚‹ç©ºã®ãƒªã‚¹ãƒˆã‚’ç”¨æ„\n",
    "        \n",
    "        for token in Tokenizer().tokenize(input_text):\n",
    "            split_token = token.part_of_speech.split(',')\n",
    "            if split_token[0] in word_class:\n",
    "                word_list.append(token.base_form)\n",
    "    \n",
    "\n",
    "        if word_list:\n",
    "            # å˜èªã®é »åº¦é›†è¨ˆ\n",
    "            df_freq = pd.DataFrame(word_list, columns=['å˜èª'])\n",
    "            df_freq['å›æ•°'] = 1  \n",
    "            df_freq_sorted = df_freq.groupby('å˜èª').sum().reset_index().sort_values('å›æ•°', ascending=False)\n",
    "\n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "            df_freq_sorted.reset_index(inplace=True, drop=True)\n",
    "            # å˜èªã®å…ˆé ­ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã‚’æŒ¿å…¥\n",
    "            df_freq_sorted['å˜èªï¼ˆã‚½ãƒ¼ãƒˆç”¨ï¼‰'] = df_freq_sorted.index.astype(str).str.zfill(2) + '_' + df_freq_sorted['å˜èª']\n",
    "            \n",
    "            ### word cloudä½œæˆ ###\n",
    "\n",
    "            # word cloudå´ã®ä»•æ§˜ã§ã€å˜èªãƒªã‚¹ãƒˆã®è¦ç´ ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã«é€£çµã™ã‚‹\n",
    "            word_space = ' '.join(map(str, word_list)) #æ•°å­—ãŒå…¥ã£ã¦ã„ãŸå ´åˆã®å¯¾ç­–ã¨ã—ã¦ã€strã«å¤‰æ›ã—ã¦ã‹ã‚‰é€£çµ\n",
    "\n",
    "            # word cloudã®è¨­å®š(ãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š)\n",
    "            wc = WordCloud(background_color='white', colormap='summer', font_path=fpath, width=800, height=400, stopwords=stop_list)\n",
    "                                                              # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ stopwords=[å˜èªãƒªã‚¹ãƒˆ]ã§é™¤å¤–å¯¾è±¡å˜èªï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼‰ã‚’è¨­å®šå¯èƒ½\n",
    "                                                              # è¨­å®šä¾‹ï¼šstopwords=['ãƒ†ãƒ¬ãƒ“', 'å•†å“']\n",
    "            wc.generate(word_space)\n",
    "            \n",
    "            ##å‡ºåŠ›ç”»åƒã®å¤§ãã•ã®æŒ‡å®š\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            ## ç›®ç››ã‚Šå‰Šé™¤ãªã©è¦‹ãŸç›®ã®ä¿®æ­£\n",
    "            plt.tick_params(labelbottom=False,\n",
    "                            labelleft=False,\n",
    "                            labelright=False,\n",
    "                            labeltop=False,\n",
    "                            length=0)\n",
    "\n",
    "            st.image(wc.to_array())\n",
    "        else:\n",
    "            st.write('æŒ‡å®šã•ã‚ŒãŸå“è©ã«ä¸€è‡´ã™ã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.96.125.141:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "2024-07-19 12:32:19.348 Uncaught app exception\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yuzawakenta/Library/Python/3.10/lib/python/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 584, in _run_script\n",
      "    exec(code, module.__dict__)\n",
      "  File \"/Users/yuzawakenta/vscode/task/classes/æƒ…å ±å‡¦ç†B/class_code/app.py\", line 182, in <module>\n",
      "    word_counts = {word: word_list.count(word) for word in target_words}\n",
      "  File \"/Users/yuzawakenta/vscode/task/classes/æƒ…å ±å‡¦ç†B/class_code/app.py\", line 182, in <dictcomp>\n",
      "    word_counts = {word: word_list.count(word) for word in target_words}\n",
      "NameError: name 'word_list' is not defined\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!echo | streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from janome.tokenizer import Tokenizer\n",
    "from wordcloud import WordCloud\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¡¨ç¤ºã™ã‚‹ãŸã‚ã®è¨­å®šå€¤\n",
    "fpath = '/System/Library/Fonts/ãƒ’ãƒ©ã‚®ãƒä¸¸ã‚´ ProN W4.ttc'\n",
    "\n",
    "def get_full_review(review_url):\n",
    "    \"\"\"Fetch the full review from the given review URL.\"\"\"\n",
    "    review_response = requests.get(review_url)\n",
    "    review_soup = BeautifulSoup(review_response.content, 'html.parser')\n",
    "    full_review_element = review_soup.find(class_='p-mark__review')\n",
    "    return full_review_element.text.strip() if full_review_element else None\n",
    "\n",
    "def scrape_reviews(page_url):\n",
    "    \"\"\"Scrape reviews from the given page URL.\"\"\"\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all elements with class 'p-mark'\n",
    "    p_mark_elements = soup.find_all(class_='p-mark')\n",
    "    \n",
    "    # Initialize lists to store scores and reviews\n",
    "    scores = []\n",
    "    reviews = []\n",
    "    \n",
    "    # Loop through each 'p-mark' element\n",
    "    for p_mark in p_mark_elements:\n",
    "        # Find the score within the 'p-mark' element\n",
    "        score_element = p_mark.find(class_='c-rating__score')\n",
    "        if score_element:\n",
    "            score_text = score_element.text.strip()\n",
    "            if score_text != '-':\n",
    "                try:\n",
    "                    score = float(score_text)\n",
    "                    scores.append(score)\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        # Find the review within the 'p-mark' element\n",
    "        review_element = p_mark.find(class_='p-mark__review')\n",
    "        if review_element:\n",
    "            # Check if there is a \"ç¶šãã‚’èª­ã‚€\" link\n",
    "            read_more_link = review_element.find('a')\n",
    "            if read_more_link and 'href' in read_more_link.attrs:\n",
    "                # Get the full review from the link\n",
    "                full_review_url = \"https://filmarks.com\" + read_more_link['href']\n",
    "                full_review = get_full_review(full_review_url)\n",
    "                reviews.append(full_review)\n",
    "            else:\n",
    "                reviews.append(review_element.text.strip())\n",
    "        else:\n",
    "            reviews.append(None)\n",
    "    \n",
    "    # Create a DataFrame from the lists of scores and reviews\n",
    "    df = pd.DataFrame({\n",
    "        'score': scores,\n",
    "        'review': reviews\n",
    "    })\n",
    "    \n",
    "    return df, soup\n",
    "\n",
    "def scrape_all_reviews(base_url):\n",
    "    # Initialize a list to store all DataFrames\n",
    "    all_dfs = []\n",
    "\n",
    "    # Start with the first page\n",
    "    page_url = base_url\n",
    "    while page_url:\n",
    "        df, soup = scrape_reviews(page_url)\n",
    "        all_dfs.append(df)\n",
    "        \n",
    "        # Find the next page link\n",
    "        next_page_element = soup.find('a', class_='c2-pagination__next')\n",
    "        if next_page_element and 'href' in next_page_element.attrs:\n",
    "            next_page_url = next_page_element['href']\n",
    "            page_url = \"https://filmarks.com\" + next_page_url\n",
    "        else:\n",
    "            page_url = None\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# Input\n",
    "base_url = st.text_input('ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„', placeholder='ï¼ˆä¾‹ï¼‰https://filmarks.com/animes/4206/5682')\n",
    "if 'data' not in st.session_state:\n",
    "    st.session_state.data = None\n",
    "\n",
    "if st.button('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’å®Ÿè¡Œ'):\n",
    "    with st.spinner('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¸­...'):\n",
    "        df = scrape_all_reviews(base_url)\n",
    "        st.session_state.data = df\n",
    "        st.success('ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†')\n",
    "\n",
    "if st.session_state.data is not None:\n",
    "    df = st.session_state.data\n",
    "    \n",
    "    score_range = st.selectbox('ã‚¹ã‚³ã‚¢ç¯„å›²ã‚’é¸æŠ', \n",
    "                               ['0-1', '1-2', '2-3', '3-4', '4-5'])\n",
    "\n",
    "    # å“è©ã‚’é¸æŠï¼ˆè¤‡æ•°é¸æŠï¼‰\n",
    "    word_class = st.multiselect('å“è©ã‚’é¸æŠ',['åè©','å½¢å®¹è©','å‹•è©','å‰¯è©'])\n",
    "\n",
    "    # æŒ‡å®šã—ãŸå˜èªã‚’ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã¨ã—ã¦é™¤å¤–\n",
    "    stop_text = st.text_input(\"ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®š\")\n",
    "    stop_list = [x.strip() for x in stop_text.split(\",\")]\n",
    "\n",
    "    # å˜èªç¾¤ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§å…¥åŠ›\n",
    "    target_words_text = st.text_input(\"ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ãªã‚‹å˜èªç¾¤ã‚’å…¥åŠ›\")\n",
    "    target_words = [x.strip() for x in target_words_text.split(\",\")]\n",
    "\n",
    "    if st.button('ãƒ‡ãƒ¼ã‚¿å‡¦ç†'):\n",
    "        # ã‚¹ã‚³ã‚¢ç¯„å›²ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°\n",
    "        score_min, score_max = map(float, score_range.split('-'))\n",
    "        filtered_df = df[(df['score'] >= score_min) & (df['score'] < score_max)]\n",
    "        \n",
    "        # Noneã‚’ç©ºæ–‡å­—åˆ—ã«ç½®ãæ›ãˆ\n",
    "        filtered_df = filtered_df.copy()\n",
    "        filtered_df.loc[:, 'review'] = filtered_df['review'].apply(lambda x: x if x is not None else '')\n",
    "        \n",
    "        # ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’é€£çµ\n",
    "        input_text = ' '.join(filtered_df['review'])\n",
    "\n",
    "        \n",
    "        word_list = [] #åˆ†å‰²å¾Œã®å½¢æ…‹ç´ ã‚’ä¸€æ¬¡æ ¼ç´ã™ã‚‹ç©ºã®ãƒªã‚¹ãƒˆã‚’ç”¨æ„\n",
    "        \n",
    "        for token in Tokenizer().tokenize(input_text):\n",
    "            split_token = token.part_of_speech.split(',')\n",
    "            if split_token[0] in word_class:\n",
    "                word_list.append(token.base_form)\n",
    "        \n",
    "\n",
    "        if target_words:\n",
    "            # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå˜èªã®å‡ºç¾å›æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "            word_counts = {word: word_list.count(word) for word in target_words}\n",
    "\n",
    "            # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¤‰æ›\n",
    "            df_counts = pd.DataFrame(list(word_counts.items()), columns=['å˜èª', 'å›æ•°'])\n",
    "\n",
    "            # æ¨ªæ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
    "            st.bar_chart(df_counts.set_index('å˜èª'))\n",
    "\n",
    "        if word_list:\n",
    "            # å˜èªã®é »åº¦é›†è¨ˆ\n",
    "            df_freq = pd.DataFrame(word_list, columns=['å˜èª'])\n",
    "            df_freq['å›æ•°'] = 1  \n",
    "            df_freq_sorted = df_freq.groupby('å˜èª').sum().reset_index().sort_values('å›æ•°', ascending=False)\n",
    "\n",
    "            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "            df_freq_sorted.reset_index(inplace=True, drop=True)\n",
    "            # å˜èªã®å…ˆé ­ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç•ªå·ã‚’æŒ¿å…¥\n",
    "            df_freq_sorted['å˜èªï¼ˆã‚½ãƒ¼ãƒˆç”¨ï¼‰'] = df_freq_sorted.index.astype(str).str.zfill(2) + '_' + df_freq_sorted['å˜èª']\n",
    "            \n",
    "            ### word cloudä½œæˆ ###\n",
    "\n",
    "            # word cloudå´ã®ä»•æ§˜ã§ã€å˜èªãƒªã‚¹ãƒˆã®è¦ç´ ã‚’ç©ºç™½åŒºåˆ‡ã‚Šã«é€£çµã™ã‚‹\n",
    "            word_space = ' '.join(map(str, word_list)) #æ•°å­—ãŒå…¥ã£ã¦ã„ãŸå ´åˆã®å¯¾ç­–ã¨ã—ã¦ã€strã«å¤‰æ›ã—ã¦ã‹ã‚‰é€£çµ\n",
    "\n",
    "            # word cloudã®è¨­å®š(ãƒ•ã‚©ãƒ³ãƒˆã®è¨­å®š)\n",
    "            wc = WordCloud(background_color='white', colormap='summer', font_path=fpath, width=800, height=400, stopwords=stop_list)\n",
    "                                                              # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ stopwords=[å˜èªãƒªã‚¹ãƒˆ]ã§é™¤å¤–å¯¾è±¡å˜èªï¼ˆã‚¹ãƒˆãƒƒãƒ—ãƒ¯ãƒ¼ãƒ‰ï¼‰ã‚’è¨­å®šå¯èƒ½\n",
    "                                                              # è¨­å®šä¾‹ï¼šstopwords=['ãƒ†ãƒ¬ãƒ“', 'å•†å“']\n",
    "            wc.generate(word_space)\n",
    "            \n",
    "            ##å‡ºåŠ›ç”»åƒã®å¤§ãã•ã®æŒ‡å®š\n",
    "            plt.figure(figsize=(10, 5))\n",
    "\n",
    "            ## ç›®ç››ã‚Šå‰Šé™¤ãªã©è¦‹ãŸç›®ã®ä¿®æ­£\n",
    "            plt.tick_params(labelbottom=False,\n",
    "                            labelleft=False,\n",
    "                            labelright=False,\n",
    "                            labeltop=False,\n",
    "                            length=0)\n",
    "\n",
    "            st.image(wc.to_array())\n",
    "        else:\n",
    "            st.write('æŒ‡å®šã•ã‚ŒãŸå“è©ã«ä¸€è‡´ã™ã‚‹å˜èªãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://10.96.125.141:8501\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  For better performance, install the Watchdog module:\u001b[0m\n",
      "\n",
      "  $ xcode-select --install\n",
      "  $ pip install watchdog\n",
      "            \u001b[0m\n",
      "^C\n",
      "\u001b[34m  Stopping...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!echo | streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
